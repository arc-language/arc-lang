package nvidia

import (
	"bytes"
	"fmt"
	"strings"

	"github.com/arc-language/arc-lang/builder/ir"
	"github.com/arc-language/arc-lang/builder/types"
)

// Generator handles the emission of NVIDIA PTX assembly
type Generator struct {
	buf      *bytes.Buffer
	regMap   map[ir.Value]string
	blockMap map[*ir.BasicBlock]string
	nextID   int
}

// Generate compiles an IR module into a PTX string
func Generate(m *ir.Module) (string, error) {
	g := &Generator{
		buf:      new(bytes.Buffer),
		regMap:   make(map[ir.Value]string),
		blockMap: make(map[*ir.BasicBlock]string),
	}

	if err := g.emitHeader(); err != nil {
		return "", err
	}

	for _, fn := range m.Functions {
		// Only process functions explicitly tagged as PTX kernels or Device functions
		if fn.CallConv != ir.CC_PTX {
			continue
		}

		if err := g.emitFunction(fn); err != nil {
			return "", err
		}
	}

	return g.buf.String(), nil
}

func (g *Generator) emitHeader() error {
	g.printf("// Generated by Arc Compiler for NVIDIA GPU\n")
	g.printf(".version 7.0\n")
	g.printf(".target sm_50\n")
	g.printf(".address_size 64\n\n")
	return nil
}

func (g *Generator) emitFunction(fn *ir.Function) error {
	g.regMap = make(map[ir.Value]string)
	g.blockMap = make(map[*ir.BasicBlock]string)
	g.nextID = 1

	for i, block := range fn.Blocks {
		g.blockMap[block] = fmt.Sprintf("$L%s_%d", fn.Name(), i)
	}

	// Always emit .entry for async gpu functions (kernels)
	g.printf(".visible .entry %s(", fn.Name())

	for i, arg := range fn.Arguments {
		ptxType := g.toPTXType(arg.Type())
		regName := fmt.Sprintf("%s_param_%d", fn.Name(), i)
		g.regMap[arg] = regName
		sep := ""
		if i < len(fn.Arguments)-1 {
			sep = ","
		}
		g.printf("\n\t.param %s %s%s", ptxType, regName, sep)
	}
	g.printf("\n) {\n")

	g.emitRegisterDeclarations(fn)

	// Load params
	for i, arg := range fn.Arguments {
		ptxType := g.toPTXType(arg.Type())
		paramName := g.regMap[arg]
		regName := fmt.Sprintf("%%r_arg%d", i)
		g.printf("\tld.param%s %s, [%s];\n", ptxType, regName, paramName)
		g.regMap[arg] = regName
	}

	for _, block := range fn.Blocks {
		g.printf("\n%s:\n", g.blockMap[block])
		for _, inst := range block.Instructions {
			if err := g.emitInstruction(inst); err != nil {
				return err
			}
		}
	}

	g.printf("}\n\n")
	return nil
}

func (g *Generator) emitRegisterDeclarations(fn *ir.Function) {
	for _, block := range fn.Blocks {
		for _, inst := range block.Instructions {
			if inst.Type() == nil || inst.Type() == types.Void {
				continue
			}
			if inst.Opcode() == ir.OpAlloca {
				// Allocas use stack memory, we register a pointer register to hold the address
				g.regMap[inst] = fmt.Sprintf("%%ptr_%d", g.nextID)
				g.nextID++
				g.printf("\t.reg .u64 %s;\n", g.regMap[inst])
				continue
			}
			
			ptxType := g.toPTXType(inst.Type())
			var prefix string
			if ptxType == ".pred" {
				prefix = "%p"
			} else if strings.HasPrefix(ptxType, ".f") {
				prefix = "%f"
			} else {
				prefix = "%r"
			}
			regName := fmt.Sprintf("%s%d", prefix, g.nextID)
			g.nextID++
			g.regMap[inst] = regName
			g.printf("\t.reg %s %s;\n", ptxType, regName)
		}
	}
}

func (g *Generator) emitInstruction(inst ir.Instruction) error {
	switch inst.Opcode() {
	// --- Arithmetic ---
	case ir.OpAdd:
		g.emitBinaryOp("add", inst)
	case ir.OpSub:
		g.emitBinaryOp("sub", inst)
	case ir.OpMul:
		if types.IsFloat(inst.Type()) {
			g.emitBinaryOp("mul", inst)
		} else {
			g.emitBinaryOp("mul.lo", inst)
		}
	case ir.OpSDiv:
		g.emitBinaryOp("div.s64", inst)
	case ir.OpUDiv:
		g.emitBinaryOp("div.u64", inst)
	case ir.OpSRem:
		g.emitBinaryOp("rem.s64", inst)
	case ir.OpURem:
		g.emitBinaryOp("rem.u64", inst)
	
	// --- Floating Point ---
	case ir.OpFAdd:
		g.emitBinaryOp("add", inst)
	case ir.OpFSub:
		g.emitBinaryOp("sub", inst)
	case ir.OpFMul:
		g.emitBinaryOp("mul", inst)
	case ir.OpFDiv:
		g.emitBinaryOp("div.rn", inst)

	// --- Bitwise ---
	case ir.OpAnd:
		g.emitBinaryOp("and.b64", inst) // PTX bitwise ops are untyped (.b)
	case ir.OpOr:
		g.emitBinaryOp("or.b64", inst)
	case ir.OpXor:
		g.emitBinaryOp("xor.b64", inst)
	case ir.OpShl:
		g.emitBinaryOp("shl.b64", inst)
	case ir.OpLShr:
		g.emitBinaryOp("shr.u64", inst) // Logical shift right
	case ir.OpAShr:
		g.emitBinaryOp("shr.s64", inst) // Arithmetic shift right

	// --- Memory ---
	case ir.OpLoad:
		dst := g.regMap[inst]
		src := g.getOperand(inst.Operands()[0])
		typ := g.toPTXType(inst.Type())
		// Use generic load (ld.s64, etc) to support stack (.local) pointers
		g.printf("\tld%s %s, [%s];\n", typ, dst, src)

	case ir.OpStore:
		val := g.getOperand(inst.Operands()[0])
		ptr := g.getOperand(inst.Operands()[1])
		typ := g.toPTXType(inst.Operands()[0].Type())
		g.printf("\tst%s [%s], %s;\n", typ, ptr, val)

	case ir.OpAlloca:
		name := fmt.Sprintf("__local_var_%d", g.nextID)
		g.nextID++
		alloca := inst.(*ir.AllocaInst)
		elemType := g.toPTXType(alloca.AllocatedType)
		// Declare local memory variable
		g.printf("\t.local %s %s;\n", elemType, name)
		// Get address into register
		dst := g.regMap[inst]
		g.printf("\tcvta.local.u64 %s, %s;\n", dst, name)

	case ir.OpGetElementPtr:
		// Basic GEP support: ptr + offset
		// Note: Robust GEP requires handling struct layouts and array indices
		// For now, we assume simple byte offset addition if possible, or error.
		dst := g.regMap[inst]
		base := g.getOperand(inst.Operands()[0])
		// This is a placeholder. Real GEP needs to calculate offset based on types.
		// For now, let's assume index 0 is 0 offset.
		g.printf("\tmov.u64 %s, %s; // GEP simplified\n", dst, base)

	// --- Comparison ---
	case ir.OpICmp:
		icmp := inst.(*ir.ICmpInst)
		dst := g.regMap[inst]
		op1 := g.getOperand(inst.Operands()[0])
		op2 := g.getOperand(inst.Operands()[1])
		typ := g.toPTXType(inst.Operands()[0].Type())
		
		cond := "eq"
		switch icmp.Predicate {
		case ir.ICmpEQ:  cond = "eq"
		case ir.ICmpNE:  cond = "ne"
		case ir.ICmpSLT: cond = "lt"
		case ir.ICmpSGT: cond = "gt"
		case ir.ICmpSLE: cond = "le"
		case ir.ICmpSGE: cond = "ge"
		}
		g.printf("\tsetp.%s%s %s, %s, %s;\n", cond, typ, dst, op1, op2)

	case ir.OpFCmp:
		fcmp := inst.(*ir.FCmpInst)
		dst := g.regMap[inst]
		op1 := g.getOperand(inst.Operands()[0])
		op2 := g.getOperand(inst.Operands()[1])
		typ := g.toPTXType(inst.Operands()[0].Type())
		
		// PTX floats use setp as well
		cond := "eq"
		switch fcmp.Predicate {
		case ir.FCmpOEQ: cond = "eq"
		case ir.FCmpONE: cond = "ne"
		case ir.FCmpOLT: cond = "lt"
		case ir.FCmpOGT: cond = "gt"
		case ir.FCmpOLE: cond = "le"
		case ir.FCmpOGE: cond = "ge"
		}
		g.printf("\tsetp.%s%s %s, %s, %s;\n", cond, typ, dst, op1, op2)

	// --- Selection (Ternary) ---
	case ir.OpSelect:
		// selp.type dst, trueVal, falseVal, predicate
		dst := g.regMap[inst]
		cond := g.getOperand(inst.Operands()[0])
		vTrue := g.getOperand(inst.Operands()[1])
		vFalse := g.getOperand(inst.Operands()[2])
		typ := g.toPTXType(inst.Type())
		g.printf("\tselp%s %s, %s, %s, %s;\n", typ, dst, vTrue, vFalse, cond)

	// --- Conversion ---
	case ir.OpSExt, ir.OpZExt, ir.OpTrunc, ir.OpFPToSI, ir.OpSIToFP:
		// cvt.destType.srcType dst, src
		dst := g.regMap[inst]
		src := g.getOperand(inst.Operands()[0])
		destTy := g.toPTXType(inst.Type())
		srcTy := g.toPTXType(inst.Operands()[0].Type())
		
		// Handle rounding modes for float conversions if needed (default usually works)
		round := ""
		if strings.Contains(destTy, ".f") || strings.Contains(srcTy, ".f") {
			round = ".rn" // Round nearest
		}
		g.printf("\tcvt%s%s%s %s, %s;\n", round, destTy, srcTy, dst, src)

	// --- Control Flow ---
	case ir.OpRet:
		g.printf("\tret;\n")

	case ir.OpBr:
		target := inst.(*ir.BrInst).Target
		label := g.blockMap[target]
		g.printf("\tbra %s;\n", label)

	case ir.OpCondBr:
		cbr := inst.(*ir.CondBrInst)
		pred := g.getOperand(cbr.Condition)
		trueLabel := g.blockMap[cbr.TrueBlock]
		falseLabel := g.blockMap[cbr.FalseBlock]
		g.printf("\t@%s bra %s;\n", pred, trueLabel)
		g.printf("\tbra %s;\n", falseLabel)

	default:
		g.printf("\t// Opcode %s not implemented\n", inst.Opcode())
	}
	return nil
}

func (g *Generator) emitBinaryOp(opcode string, inst ir.Instruction) {
	dst := g.regMap[inst]
	src1 := g.getOperand(inst.Operands()[0])
	src2 := g.getOperand(inst.Operands()[1])
	suffix := ""
	if !strings.Contains(opcode, ".") {
		suffix = g.toPTXType(inst.Type())
	}
	g.printf("\t%s%s %s, %s, %s;\n", opcode, suffix, dst, src1, src2)
}

func (g *Generator) getOperand(v ir.Value) string {
	if c, ok := v.(*ir.ConstantInt); ok {
		return fmt.Sprintf("%d", c.Value)
	}
	if c, ok := v.(*ir.ConstantFloat); ok {
		return fmt.Sprintf("%f", c.Value)
	}
	if reg, ok := g.regMap[v]; ok {
		return reg
	}
	return "%unknown"
}

func (g *Generator) toPTXType(t types.Type) string {
	if t == nil {
		return ".void"
	}
	if _, ok := t.(*types.PointerType); ok {
		return ".u64"
	}
	switch t.Kind() {
	case types.IntegerKind:
		bits := t.(*types.IntType).BitWidth
		if bits == 1 {
			return ".pred"
		}
		// For this simple generator, we treat standard Ints as signed
		// and explicit Uint logic would likely need IR Type differentiation.
		return fmt.Sprintf(".s%d", bits)
	case types.FloatKind:
		bits := t.(*types.FloatType).BitWidth
		return fmt.Sprintf(".f%d", bits)
	case types.VoidKind:
		return ".void"
	default:
		return ".b64"
	}
}

func (g *Generator) printf(format string, args ...interface{}) {
	fmt.Fprintf(g.buf, format, args...)
}